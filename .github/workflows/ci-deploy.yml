name: Deploy Memos to GitHub Pages

on:
  push:
    branches: [ main ]
    paths:
      - 'memos/**'
      - 'uploads/**'
      - 'notebooks/**'
      - 'docs/notebooks.json'
      - 'docs/assets/js/**'
      - 'docs/index.html'
      - '.github/workflows/ci-deploy.yml'

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repo
      uses: actions/checkout@v3
      with:
        fetch-depth: 0

    - name: Install TeX Live & unzip
      run: |
        sudo apt-get update
        sudo apt-get install -y texlive-latex-extra texlive-publishers latexmk unzip

    - name: Unzip memo ZIPs
      run: |
        # Unzip any ZIP files placed directly in memos/
        for z in memos/*.zip; do
          if [[ -f "$z" ]]; then
            name=$(basename "$z" .zip)
            mkdir -p "memos/$name"
            echo "Unzipping $z to memos/$name/"
            unzip -o "$z" -d "memos/$name"
          fi
        done
        # Unzip any ZIPs inside memo subfolders
        for dir in memos/*/ ; do
          for z in "$dir"/*.zip; do
            if [[ -f "$z" ]]; then
              echo "Unzipping $z to $dir"
              unzip -o "$z" -d "$dir"
            fi
          done
        done

    - name: Compile LaTeX memos
      run: |
        mkdir -p docs/pdfs
        # Compile each memo directory, naming PDF after the folder
        for dir in memos/*/ ; do
          name=$(basename "$dir")
          echo "Processing memo $name"
          (
            cd "$dir"
            texfile=$(ls *.tex | head -n1)
            echo "Compiling $texfile as $name.pdf"
            latexmk -pdf \
              -jobname="$name" \
              -output-directory="../../docs/pdfs" \
              "$texfile"
          )
        done
        # Clean auxiliary files
        find docs/pdfs -type f ! -name '*.pdf' -delete

    - name: Copy direct PDF uploads
      run: |
        cp uploads/*.pdf docs/pdfs/ || echo "No uploads to copy"

    - name: Generate memo catalog with persistent numbering
      run: |
        python - <<'PY'
        import json, os, re, subprocess
        from datetime import datetime, timezone
        from pathlib import Path

        REPO = Path(".").resolve()
        MEMOS_DIR = REPO / "memos"
        CATALOG_FILE = REPO / "docs" / "memo_catalog.json"

        def git_first_commit_date(path: Path) -> str:
          """Get the date of the FIRST commit that touched this path"""
          try:
            iso = subprocess.check_output(
              ["git", "log", "--follow", "--diff-filter=A", "--format=%cI", "--", str(path)],
              text=True
            ).strip().split('\n')[-1]  # Get the last line (oldest commit)
            return iso or datetime.now(timezone.utc).isoformat()
          except Exception:
            return datetime.now(timezone.utc).isoformat()

        def parse_latex_metadata(tex_path: Path):
          """Extract title, authors, tags from LaTeX comments or commands"""
          metadata = {"title": tex_path.stem, "authors": [], "tags": [], "summary": ""}
          
          try:
            content = tex_path.read_text(encoding='utf-8', errors='ignore')
            
            # Look for % TITLE: ... or \title{...}
            title_match = re.search(r'%\s*TITLE:\s*(.+)', content) or \
                         re.search(r'\\title\{([^}]+)\}', content)
            if title_match:
              metadata["title"] = title_match.group(1).strip()
            
            # Look for % AUTHORS: ... or \author{...}
            authors_match = re.search(r'%\s*AUTHORS?:\s*(.+)', content) or \
                           re.search(r'\\author\{([^}]+)\}', content)
            if authors_match:
              authors_str = authors_match.group(1).strip()
              metadata["authors"] = [a.strip() for a in re.split(r'[,&]|\sand\s', authors_str) if a.strip()]
            
            # Look for % TAGS: ... 
            tags_match = re.search(r'%\s*TAGS?:\s*(.+)', content)
            if tags_match:
              tags_str = tags_match.group(1).strip()
              metadata["tags"] = [t.strip() for t in tags_str.split(',') if t.strip()]
            
            # Look for % SUMMARY: ...
            summary_match = re.search(r'%\s*SUMMARY:\s*(.+)', content)
            if summary_match:
              metadata["summary"] = summary_match.group(1).strip()
              
          except Exception as e:
            print(f"Warning: couldn't parse {tex_path}: {e}")
          
          return metadata

        # Load existing catalog if it exists
        existing_catalog = {}
        if CATALOG_FILE.exists():
          try:
            with open(CATALOG_FILE, 'r', encoding='utf-8') as f:
              catalog_data = json.load(f)
              existing_catalog = {item['slug']: item for item in catalog_data}
              print(f"Loaded existing catalog with {len(existing_catalog)} entries")
          except Exception as e:
            print(f"Warning: couldn't load existing catalog: {e}")

        # Collect all current memos
        current_memos = []
        if MEMOS_DIR.exists():
          for memo_dir in sorted(MEMOS_DIR.iterdir()):
            if not memo_dir.is_dir():
              continue
            
            # Find the .tex file
            tex_files = list(memo_dir.glob("*.tex"))
            if not tex_files:
              continue
            
            tex_path = tex_files[0]
            slug = memo_dir.name
            
            # Check if this memo already has a number in the catalog
            if slug in existing_catalog:
              # Keep existing number and first_commit_date
              memo_number = existing_catalog[slug]['number']
              first_commit = existing_catalog[slug]['first_commit_date']
              print(f"Memo {slug} keeps existing number: {memo_number}")
            else:
              # New memo - will assign number later
              first_commit = git_first_commit_date(memo_dir)
              memo_number = None
              print(f"New memo {slug} with first commit: {first_commit}")
            
            metadata = parse_latex_metadata(tex_path)
            
            current_memos.append({
              "slug": slug,
              "title": metadata["title"],
              "authors": metadata["authors"],
              "tags": metadata["tags"],
              "summary": metadata["summary"],
              "first_commit_date": first_commit,
              "number": memo_number
            })

        # Separate memos with and without numbers
        numbered_memos = [m for m in current_memos if m['number'] is not None]
        new_memos = [m for m in current_memos if m['number'] is None]

        # Sort new memos by first_commit_date, then alphabetically by slug
        new_memos.sort(key=lambda m: (m['first_commit_date'], m['slug']))

        # Assign numbers to new memos
        if numbered_memos:
          next_number = max(m['number'] for m in numbered_memos) + 1
        else:
          next_number = 1

        for memo in new_memos:
          memo['number'] = next_number
          print(f"Assigned number {next_number} to {memo['slug']}")
          next_number += 1

        # Combine all memos and sort by number
        all_memos = numbered_memos + new_memos
        all_memos.sort(key=lambda m: m['number'])

        # Save catalog
        CATALOG_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(CATALOG_FILE, "w", encoding="utf-8") as f:
          json.dump(all_memos, f, indent=2, ensure_ascii=False)
        print(f"Wrote {CATALOG_FILE} with {len(all_memos)} memos")
        PY

    - name: Generate docs/memos.json from catalog
      run: |
        python - <<'PY'
        import json
        from pathlib import Path
        import subprocess
        from datetime import datetime, timezone

        REPO = Path(".").resolve()
        CATALOG_FILE = REPO / "docs" / "memo_catalog.json"
        MANIFEST = REPO / "docs" / "memos.json"

        def git_last_modified(path: Path) -> str:
          """Get the date of the LAST modification"""
          try:
            iso = subprocess.check_output(
              ["git", "log", "-1", "--format=%cI", "--", str(path)],
              text=True
            ).strip()
            return iso or datetime.now(timezone.utc).isoformat()
          except Exception:
            return datetime.now(timezone.utc).isoformat()

        # Load catalog
        with open(CATALOG_FILE, 'r', encoding='utf-8') as f:
          catalog = json.load(f)

        # Create memos.json with additional fields
        items = []
        for memo in catalog:
          memo_dir = REPO / "memos" / memo['slug']
          pdf_name = memo['slug'] + ".pdf"
          
          items.append({
            "slug": memo['slug'],
            "number": memo['number'],
            "title": memo['title'],
            "date": git_last_modified(memo_dir),  # Last modified date for sorting
            "first_commit_date": memo['first_commit_date'],  # Original commit date
            "authors": memo['authors'],
            "tags": memo['tags'],
            "pdf": f"/pdfs/{pdf_name}",
            "summary": memo['summary'],
            "kind": "memo"
          })

        with open(MANIFEST, "w", encoding="utf-8") as f:
          json.dump(items, f, indent=2, ensure_ascii=False)
        print(f"Wrote {MANIFEST} with {len(items)} memos")
        PY

    - name: Set up Python for notebook build
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install nbconvert toolchain
      run: |
        python -m pip install --upgrade pip
        pip install nbconvert==7.* nbformat==5.* jinja2==3.* pygments==2.* pandocfilters

    - name: Convert .ipynb to HTML
      run: |
        set -e
        mkdir -p docs/notebooks
        shopt -s nullglob
        for nb in notebooks/*.ipynb; do
          echo "Converting $nb"
          jupyter nbconvert --to html --output-dir "docs/notebooks" "$nb"
        done

    - name: Generate docs/notebooks.json
      run: |
        python - <<'PY'
        import json, os, re, subprocess, sys
        from datetime import datetime, timezone
        from pathlib import Path
        import nbformat

        REPO = Path(".").resolve()
        SRC_DIR = REPO / "notebooks"
        OUT_DIR = REPO / "docs" / "notebooks"
        MANIFEST = REPO / "docs" / "notebooks.json"

        def git_last_modified(path: Path) -> str:
          try:
            iso = subprocess.check_output(
              ["git", "log", "-1", "--format=%cI", "--", str(path)],
              text=True
            ).strip()
            return iso or datetime.now(timezone.utc).isoformat()
          except Exception:
            return datetime.now(timezone.utc).isoformat()

        def first_markdown_h1(nb):
          for cell in nb.cells:
            if cell.cell_type == "markdown":
              for line in cell.source.splitlines():
                if line.startswith("# "):
                  return line[2:].strip()
          return None

        def meta_list(meta, key):
          v = meta.get(key)
          if isinstance(v, list):
            return v
          if isinstance(v, str) and v.strip():
            return [v.strip()]
          return []

        items = []
        if SRC_DIR.exists():
          for ipynb in sorted(SRC_DIR.glob("*.ipynb")):
            try:
              nb = nbformat.read(str(ipynb), as_version=4)
            except Exception:
              nb = None

            title = None
            authors = []
            tags = []

            if nb:
              title = (nb.metadata.get("title") or first_markdown_h1(nb) or ipynb.stem).strip()
              authors = meta_list(nb.metadata, "authors")
              tags = meta_list(nb.metadata, "tags")

            href = f"/notebooks/{ipynb.with_suffix('.html').name}"
            date_iso = git_last_modified(ipynb)
            summary = nb.metadata.get("summary") if nb and "summary" in nb.metadata else ""

            items.append({
              "title": title or ipynb.stem,
              "date": date_iso,
              "authors": authors,
              "tags": tags,
              "href": href,
              "source": f"/{ipynb.as_posix()}",
              "kind": "notebook",
              "summary": summary,
            })

        MANIFEST.parent.mkdir(parents=True, exist_ok=True)
        with open(MANIFEST, "w", encoding="utf-8") as f:
          json.dump(items, f, indent=2, ensure_ascii=False)
        print(f"Wrote {MANIFEST} with {len(items)} item(s).")
        PY

    - name: Commit generated artifacts
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add docs/notebooks docs/notebooks.json docs/memos.json docs/memo_catalog.json docs/pdfs
        git commit -m "rebuild notebooks, memos, pdfs & manifests" || echo "No changes to commit"

    - name: Push to GitHub
      uses: ad-m/github-push-action@v0.6.0
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        branch: main
